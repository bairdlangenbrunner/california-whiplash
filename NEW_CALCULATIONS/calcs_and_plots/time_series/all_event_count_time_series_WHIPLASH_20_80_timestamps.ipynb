{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import netCDF4\n",
    "import matplotlib.pyplot as mp\n",
    "import matplotlib.ticker\n",
    "import matplotlib.colors\n",
    "import scipy.stats\n",
    "import pandas\n",
    "import itertools\n",
    "from mpl_toolkits import basemap\n",
    "import mpl_toolkits.axes_grid1\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "degree_sign = u'\\u00B0'\n",
    "mp.rcParams.update({'mathtext.default': 'regular'})\n",
    "mp.rcParams['hatch.color'] = '0.5'\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/npy_files/'\n",
    "#save_dir = '/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/calculations/npy_files/'\n",
    "#latlon_indices = numpy.load(working_dir + 'ccal_latlon_indices_array.npy'); region='ccal'\n",
    "#latlon_indices = numpy.load(working_dir + 'ncal_latlon_indices_array.npy'); region='ncal'\n",
    "#latlon_indices = numpy.load(working_dir + 'scal_latlon_indices_array.npy'); region='scal'\n",
    "\n",
    "PRECT_lat = numpy.load('/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/npy_files/PRECT_lat.npy')\n",
    "PRECT_lon = numpy.load('/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/npy_files/PRECT_lon.npy')\n",
    "\n",
    "PRECT_nlat = 26\n",
    "PRECT_nlon = 25\n",
    "\n",
    "latlon_indices = list(itertools.product(range(PRECT_nlat), range(PRECT_nlon)))\n",
    "region = 'whole_domain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_lat = 34.0522\n",
    "LA_lon = 118.2437 # deg west\n",
    "LA_lon = 180. + (180-LA_lon)\n",
    "\n",
    "Oroville_dam_lat = 39.5380\n",
    "Oroville_dam_lon = 121.4831 # deg west\n",
    "Oroville_dam_lon = 360 - Oroville_dam_lon\n",
    "\n",
    "SF_lat = 37.7749\n",
    "SF_lon = 122.4194\n",
    "SF_lon = 360 - SF_lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open coastal lat/lon indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_coast_latlon_indices = numpy.load('/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/npy_files/coastal_latlon_array_indices_3x.npy')\n",
    "cali_coast_latlon_indices_zip = [i for i in zip(cali_coast_latlon_indices[:,0], cali_coast_latlon_indices[:,1])]\n",
    "\n",
    "cali_coast_latlon_indices_3d = cali_coast_latlon_indices.reshape((11,3,2))\n",
    "#print(cali_coast_latlon_indices_3d[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_idx(dd, dd_array):\n",
    "   \"\"\"\n",
    "     search for nearest decimal degree in an array of decimal degrees and return the index.\n",
    "     np.argmin returns the indices of minium value along an axis.\n",
    "     so subtract dd from all values in dd_array, take absolute value and find index of minium.\n",
    "    \"\"\"\n",
    "   geo_idx = (numpy.abs(dd_array - dd)).argmin()\n",
    "   return geo_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_lat_idx = geo_idx(LA_lat, PRECT_lat)\n",
    "LA_lon_idx = geo_idx(LA_lon, PRECT_lon)\n",
    "\n",
    "SF_lat_idx = geo_idx(SF_lat, PRECT_lat)\n",
    "SF_lon_idx = geo_idx(SF_lon, PRECT_lon)\n",
    "\n",
    "OD_lat_idx = geo_idx(Oroville_dam_lat, PRECT_lat)\n",
    "OD_lon_idx = geo_idx(Oroville_dam_lon, PRECT_lon)\n",
    "\n",
    "#REGION_lat_idx, REGION_lon_idx = SF_lat_idx, SF_lon_idx; REGION_NAME = 'SF'\n",
    "REGION_lat_idx, REGION_lon_idx = LA_lat_idx, LA_lon_idx; REGION_NAME = 'LA'\n",
    "#REGION_lat_idx, REGION_lon_idx = OD_lat_idx, OD_lon_idx; REGION_NAME = 'OD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start_pic = 402 #time_subsets[chunk,0]\n",
    "year_end_pic = 2200 #time_subsets[chunk,1]\n",
    "\n",
    "# create season strings\n",
    "years_pic = numpy.arange(year_start_pic, year_end_pic+1, 1).astype(numpy.int)\n",
    "half_years_pic = numpy.arange(year_start_pic+0.75, year_end_pic, 1)\n",
    "#season_strings_pic = numpy.empty(years.size-1, dtype=numpy.str)\n",
    "\n",
    "season_strings_pic = [str(years_pic[i])+'-'+str(years_pic[i+1]) for i in range(years_pic.size-1)]\n",
    "member_strings_pic = ['{:03d}'.format(i) for i in range(1,36)]\n",
    "\n",
    "n_seasons_pic=year_end_pic-year_start_pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_members = numpy.hstack((numpy.arange(1,36), numpy.arange(101,106)))\n",
    "ensemble_names = ['{:03d}'.format(i) for i in ensemble_members]\n",
    "\n",
    "year_start = 1920\n",
    "year_end = 2100\n",
    "\n",
    "year_start_list = numpy.arange(1920,2070)\n",
    "year_end_list = numpy.arange(1950,2100)\n",
    "year_middle_list = year_start_list+15\n",
    "\n",
    "# create season strings\n",
    "years = numpy.arange(year_start, year_end+1, 1).astype(numpy.int)\n",
    "half_years_hist_rcp = numpy.arange(year_start+0.75, year_end, 1)\n",
    "season_strings_hist_rcp = [str(i)+'-'+str(i+1) for i in range(year_start,year_end)]\n",
    "\n",
    "thirty_yr_strings_hist_rcp = [str(year_start_list[i])+'-'+str(year_end_list[i]) for i in range(year_start_list.size)]\n",
    "member_strings_hist_rcp = ['{:03d}'.format(i) for i in range(1,36)]\n",
    "\n",
    "n_seasons_hist_rcp=year_end-year_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary to store everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# do seasonal whiplash calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/calcs_and_plots/whiplash/calculations/time_series_hist_rcp_dry20_wet80_events.nc'\n",
    "ncfile = netCDF4.Dataset(filename)\n",
    "hist_rcp_dry_events = ncfile.variables['all_dry_events'][:]\n",
    "hist_rcp_wet_events = ncfile.variables['all_wet_events'][:]\n",
    "hist_rcp_time_var = ncfile.variables['time']\n",
    "hist_rcp_time_data = ncfile.variables['time'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rcp_time_datetime = netCDF4.num2date(hist_rcp_time_data, units=hist_rcp_time_var.units, calendar='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1921, 1, 15, 0, 0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rcp_time_datetime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/calcs_and_plots/whiplash/calculations/seasonal_whiplash_PIC_low_to_high.nc'\n",
    "ncfile = netCDF4.Dataset(filename, 'r', 'NetCDF4')\n",
    "whiplash_events_pic = ncfile.variables['whiplash_events'][:]\n",
    "pic_time_var = ncfile.variables['time']\n",
    "pic_time_data = ncfile.variables['time'][:]\n",
    "\n",
    "filename = '/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/calcs_and_plots/whiplash/calculations/seasonal_whiplash_hist_rcp_low_to_high.nc'\n",
    "ncfile = netCDF4.Dataset(filename, 'r', 'NetCDF4')\n",
    "whiplash_events_hist_rcp = ncfile.variables['seasonal_total'][:]\n",
    "hist_rcp_time_var = ncfile.variables['time']\n",
    "hist_rcp_time_data = ncfile.variables['time'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_time_datetime = netCDF4.num2date(pic_time_data, pic_time_var.units, 'standard')\n",
    "hist_rcp_time_datetime = netCDF4.num2date(hist_rcp_time_data, hist_rcp_time_var.units, 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2100, 1, 15, 0, 0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_rcp_time_datetime[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiplash_count_pic = numpy.sum(whiplash_events_pic==1, axis=0)\n",
    "whiplash_count_pic_mean = numpy.array(whiplash_count_pic)/1798.\n",
    "\n",
    "# sum hist+rcp across ensemble members, then divide by 40\n",
    "whiplash_count_hist_rcp = numpy.sum(whiplash_events_hist_rcp==1, axis=1)\n",
    "whiplash_count_hist_rcp_mean = numpy.array(whiplash_count_hist_rcp)/40.\n",
    "\n",
    "whiplash_ratios_change = 100*numpy.array((whiplash_count_hist_rcp==1) - whiplash_count_pic_mean)/(whiplash_count_pic_mean)\n",
    "whiplash_ratios_mean = 100*whiplash_count_hist_rcp_mean/whiplash_count_pic_mean\n",
    "whiplash_ratios = whiplash_count_hist_rcp/whiplash_count_pic_mean\n",
    "#whiplash_ratios = numpy.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rcp_counts_ALL_LATLON_smooth = numpy.zeros((whiplash_count_hist_rcp.shape))\n",
    "for i in range(PRECT_nlat):\n",
    "    for j in range(PRECT_nlon):\n",
    "        hist_rcp_counts_ALL_LATLON_smooth[:,i,j] = pandas.Series(whiplash_count_hist_rcp[:,i,j]).rolling(window=window, center=True).mean()\n",
    "\n",
    "pic_counts_ALL_LATLON_equivalent = whiplash_count_pic/(1798/40)\n",
    "hist_rcp_counts_ALL_LATLON_smooth_norm = numpy.zeros((whiplash_count_hist_rcp.shape))\n",
    "for i in range(PRECT_nlat):\n",
    "    for j in range(PRECT_nlon):\n",
    "        hist_rcp_counts_ALL_LATLON_smooth_norm[:,i,j] = (hist_rcp_counts_ALL_LATLON_smooth[:,i,j] - pic_counts_ALL_LATLON_equivalent[i,j])/pic_counts_ALL_LATLON_equivalent[i,j] * 100.\n",
    "\n",
    "hist_rcp_counts_ALL_LATLON_smooth_ratios = numpy.zeros((whiplash_count_hist_rcp.shape))\n",
    "for i in range(PRECT_nlat):\n",
    "    for j in range(PRECT_nlon):\n",
    "        hist_rcp_counts_ALL_LATLON_smooth_ratios[:,i,j] = hist_rcp_counts_ALL_LATLON_smooth[:,i,j]/pic_counts_ALL_LATLON_equivalent[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 25)\n"
     ]
    }
   ],
   "source": [
    "count_ratio_2d = hist_rcp_counts_ALL_LATLON_smooth_ratios[-15,:,:]\n",
    "print(count_ratio_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rcp_counts_ALL_LATLON_smooth_ratios = hist_rcp_counts_ALL_LATLON_smooth_ratios.transpose(1,2,0)\n",
    "events_dict['whiplash_pic_count'] = whiplash_count_pic\n",
    "events_dict['whiplash_2d_frequency_ratios_30yr_avg'] = hist_rcp_counts_ALL_LATLON_smooth_ratios[:,:,-15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 25, 180)\n"
     ]
    }
   ],
   "source": [
    "print(hist_rcp_counts_ALL_LATLON_smooth_ratios.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 25)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_dict['whiplash_2d_frequency_ratios_30yr_avg'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save netcdf file for maps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "units_list = ['30yr mean frequency ratio (2070-2100 vs PIC)',\\\n",
    "              'P-values from bootstrap distribution'] * 6\n",
    "\n",
    "keys_save_2d = [\\\n",
    "'100yr_3season_dry_2d_frequency_ratios_30yr_avg', \\\n",
    "'100yr_3season_dry_pvals', \\\n",
    "\n",
    "'100yr_1season_dry_2d_frequency_ratios_30yr_avg', \\\n",
    "'100yr_1season_dry_pvals', \\\n",
    "\n",
    "'25yr_1season_wet_2d_frequency_ratios_30yr_avg', \\\n",
    "'25yr_1season_wet_pvals', \\\n",
    "\n",
    "'200yr_40d_wet_2d_frequency_ratios_30yr_avg', \\\n",
    "'200yr_40d_wet_pvals', \\\n",
    "\n",
    "'whiplash_2d_frequency_ratios_30yr_avg', \\\n",
    "'whiplash_pvals', \\\n",
    "\n",
    "'whiplash_monthly_2d_frequency_ratios_30yr_avg', \\\n",
    "'whiplash_monthly_pvals' ]\n",
    "\n",
    "\n",
    "# save hist clim\n",
    "filename = 'frequency_change_maps_and_sig_vals_'+'LAT_LON'+'_with_whiplash.nc'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print('file exists')\n",
    "    os.remove(filename)\n",
    "\n",
    "ncfile = netCDF4.Dataset(filename, 'w', format='NETCDF4')\n",
    "\n",
    "lat_dim = ncfile.createDimension('lat', PRECT_nlat)\n",
    "lat_var = ncfile.createVariable('lat', 'f4', ('lat',))\n",
    "lat_var[:] = PRECT_lat\n",
    "lat_var.units = 'degrees North'\n",
    "\n",
    "lon_dim = ncfile.createDimension('lon', PRECT_nlon)\n",
    "lon_var = ncfile.createVariable('lon', 'f4', ('lon',))\n",
    "lon_var[:] = PRECT_lon\n",
    "lon_var.units = 'degrees East'\n",
    "\n",
    "for key in range(len(keys_save_2d)):\n",
    "    variable = ncfile.createVariable(keys_save_2d[key], 'f4', ('lat', 'lon',), fill_value=numpy.nan)\n",
    "    print(keys_save_2d[key])\n",
    "    print(events_dict[keys_save_2d[key]].shape)\n",
    "    variable[:] = events_dict[keys_save_2d[key]]\n",
    "    variable.units = units_list[key]\n",
    "\n",
    "ncfile.history = 'Created ' + time.ctime(time.time())\n",
    "ncfile.close()\n",
    "print(filename, \"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save time-latitude files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert times\n",
    "year_list = numpy.arange(1921,2101)\n",
    "time_datetime = [datetime.datetime(i,1,15) for i in year_list]\n",
    "time_nc = netCDF4.date2num(time_datetime, units='days since 1920-01-01', calendar='standard')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PRECT_lat_subset = PRECT_lat[numpy.unique(cali_coast_latlon_indices[:,0])]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "units_list = ['30yr running mean frequency ratio (time-latitude)',\\\n",
    "              'P-values from bootstrap distribution'] * 6\n",
    "\n",
    "keys_save_2d = [\\\n",
    "'100yr_3season_dry_timelat_ratios', \\\n",
    "'100yr_3season_dry_timelat_pvals', \\\n",
    "\n",
    "'100yr_1season_dry_timelat_ratios', \\\n",
    "'100yr_1season_dry_timelat_pvals', \\\n",
    "\n",
    "'25yr_1season_wet_timelat_ratios', \\\n",
    "'25yr_1season_wet_timelat_pvals', \\\n",
    "\n",
    "'200yr_40d_wet_timelat_ratios', \\\n",
    "'200yr_40d_wet_timelat_pvals', \\\n",
    "\n",
    "'whiplash_timelat_ratios', \\\n",
    "'whiplash_timelat_pvals', \\\n",
    "\n",
    "'whiplash_monthly_timelat_ratios', \\\n",
    "'whiplash_monthly_timelat_pvals' ]\n",
    "\n",
    "# save hist clim\n",
    "filename = 'frequency_change_maps_and_sig_vals_'+'TIME_LATITUDE'+'_with_whiplash.nc'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print('file exists')\n",
    "    os.remove(filename)\n",
    "\n",
    "ncfile = netCDF4.Dataset(filename, 'w', format='NETCDF4')\n",
    "\n",
    "lat_dim = ncfile.createDimension('lat', PRECT_lat_subset.size)\n",
    "time_dim = ncfile.createDimension('time', len(year_list))\n",
    "\n",
    "lat_var = ncfile.createVariable('lat', 'f4', ('lat',))\n",
    "time_var = ncfile.createVariable('time', 'f4', ('time',))\n",
    "lat_var[:] = PRECT_lat_subset\n",
    "time_var[:] = time_nc\n",
    "lat_var.units = 'degrees_north'\n",
    "time_var.units = 'days since 1920-01-01'\n",
    "time_var.description = 'Reported for yearly (seasonal) values; no smoothing or averaging'\n",
    "\n",
    "for key in range(len(keys_save_2d)):\n",
    "    variable = ncfile.createVariable(keys_save_2d[key], 'f4', ('lat', 'time',), fill_value=numpy.nan)\n",
    "    print(keys_save_2d[key])\n",
    "    print(events_dict[keys_save_2d[key]].shape)\n",
    "    variable[:] = events_dict[keys_save_2d[key]]\n",
    "    variable.units = units_list[key]\n",
    "\n",
    "ncfile.history = 'Created ' + time.ctime(time.time())\n",
    "ncfile.close()\n",
    "print(filename, \"saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
