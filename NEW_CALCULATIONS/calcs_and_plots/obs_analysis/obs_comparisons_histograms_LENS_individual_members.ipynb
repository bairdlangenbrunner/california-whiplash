{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import netCDF4\n",
    "import matplotlib.pyplot as mp\n",
    "import matplotlib.ticker\n",
    "import matplotlib.colors\n",
    "import scipy.stats\n",
    "import pandas\n",
    "import itertools\n",
    "import datetime\n",
    "import os\n",
    "import geopy\n",
    "import calendar\n",
    "\n",
    "import cartopy\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start = 1979\n",
    "year_end = 2016\n",
    "\n",
    "ys_str = str(year_start+1)\n",
    "ye_str = str(year_end-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_lat = 34.0522\n",
    "LA_lon = 118.2437 # deg west\n",
    "LA_lon = 180. + (180-LA_lon)\n",
    "\n",
    "Oroville_dam_lat = 39.5380\n",
    "Oroville_dam_lon = 121.4831 # deg west\n",
    "Oroville_dam_lon = 360 - Oroville_dam_lon\n",
    "\n",
    "SF_lat = 37.7749\n",
    "SF_lon = 122.4194\n",
    "SF_lon = 360 - SF_lon\n",
    "\n",
    "def geo_idx(dd, dd_array):\n",
    "   \"\"\"\n",
    "     search for nearest decimal degree in an array of decimal degrees and return the index.\n",
    "     np.argmin returns the indices of minium value along an axis.\n",
    "     so subtract dd from all values in dd_array, take absolute value and find index of minium.\n",
    "    \"\"\"\n",
    "   geo_idx = (numpy.abs(dd_array - dd)).argmin()\n",
    "   return geo_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_shapes = cartopy.io.shapereader.natural_earth(resolution='110m', category='cultural', name='admin_1_states_provinces')\n",
    "shapes_reader = cartopy.io.shapereader.Reader(states_shapes)\n",
    "\n",
    "states_name_list = numpy.array(([i.attributes['name'] for i in shapes_reader.records()]))\n",
    "for i in shapes_reader.records():\n",
    "    if i.attributes['name']=='California' and i.attributes['iso_a2']=='US':\n",
    "        CA_object = i\n",
    "CA_object_no_islands = CA_object.geometry.geoms[-1] # get the last one (the state of Cali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open seasonal totals from LENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rcp_filename = '/Users/baird/Dropbox/_analysis/attribution_2017/NEW_CALCULATIONS/calcs_and_plots/create_ncfiles_seasonal/seasonal_totals_hist_rcp.nc'\n",
    "\n",
    "hist_rcp_ncfile = netCDF4.Dataset(hist_rcp_filename)\n",
    "hist_rcp_data = hist_rcp_ncfile.variables['seasonal_total']\n",
    "hist_rcp_time_var = hist_rcp_ncfile.variables['time']\n",
    "hist_rcp_time = hist_rcp_time_var[:]\n",
    "PRECT_lat = hist_rcp_ncfile.variables['lat'][:]\n",
    "PRECT_lon = hist_rcp_ncfile.variables['lon'][:]\n",
    "\n",
    "time_datetimes = netCDF4.num2date(hist_rcp_time, hist_rcp_time_var.units, 'standard')\n",
    "time_datetimes_subset = [t.year in range(year_start,year_end+1) for t in time_datetimes]\n",
    "hist_rcp_data = hist_rcp_data[time_datetimes_subset,:,:,:]\n",
    "time_datetimes = time_datetimes[time_datetimes_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_lat_idx = geo_idx(LA_lat, PRECT_lat)\n",
    "LA_lon_idx = geo_idx(LA_lon, PRECT_lon)\n",
    "\n",
    "SF_lat_idx = geo_idx(SF_lat, PRECT_lat)\n",
    "SF_lon_idx = geo_idx(SF_lon, PRECT_lon)\n",
    "\n",
    "OD_lat_idx = geo_idx(Oroville_dam_lat, PRECT_lat)\n",
    "OD_lon_idx = geo_idx(Oroville_dam_lon, PRECT_lon)\n",
    "\n",
    "#REGION_lat_idx, REGION_lon_idx = SF_lat_idx, SF_lon_idx; REGION_NAME = 'SF'\n",
    "REGION_lat_idx, REGION_lon_idx = LA_lat_idx, LA_lon_idx; REGION_NAME = 'LA'\n",
    "#REGION_lat_idx, REGION_lon_idx = OD_lat_idx, OD_lon_idx; REGION_NAME = 'OD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_combos_lens = numpy.array((list(itertools.product(range(PRECT_lat.size),range(PRECT_lon.size))) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=300\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = PRECT_lat[LA_lat_idx],PRECT_lon[LA_lon_idx]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos_lens.__len__()):\n",
    "    lat_new,lon_new = PRECT_lat[latlon_combos_lens[latlon][0]], -(360-PRECT_lon[latlon_combos_lens[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_lens_300km = distance_array_ledist.reshape((PRECT_lat.size,PRECT_lon.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = PRECT_lat[SF_lat_idx],PRECT_lon[SF_lon_idx]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos_lens.__len__()):\n",
    "    lat_new,lon_new = PRECT_lat[latlon_combos_lens[latlon][0]], -(360-PRECT_lon[latlon_combos_lens[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_lens_300km = distance_array_ledist.reshape((PRECT_lat.size,PRECT_lon.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=200\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = PRECT_lat[LA_lat_idx],PRECT_lon[LA_lon_idx]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos_lens.__len__()):\n",
    "    lat_new,lon_new = PRECT_lat[latlon_combos_lens[latlon][0]], -(360-PRECT_lon[latlon_combos_lens[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_lens_200km = distance_array_ledist.reshape((PRECT_lat.size,PRECT_lon.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = PRECT_lat[SF_lat_idx],PRECT_lon[SF_lon_idx]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos_lens.__len__()):\n",
    "    lat_new,lon_new = PRECT_lat[latlon_combos_lens[latlon][0]], -(360-PRECT_lon[latlon_combos_lens[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_lens_200km = distance_array_ledist.reshape((PRECT_lat.size,PRECT_lon.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=110\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = PRECT_lat[LA_lat_idx],PRECT_lon[LA_lon_idx]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos_lens.__len__()):\n",
    "    lat_new,lon_new = PRECT_lat[latlon_combos_lens[latlon][0]], -(360-PRECT_lon[latlon_combos_lens[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_lens_110km = distance_array_ledist.reshape((PRECT_lat.size,PRECT_lon.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = PRECT_lat[SF_lat_idx],PRECT_lon[SF_lon_idx]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos_lens.__len__()):\n",
    "    lat_new,lon_new = PRECT_lat[latlon_combos_lens[latlon][0]], -(360-PRECT_lon[latlon_combos_lens[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_lens_110km = distance_array_ledist.reshape((PRECT_lat.size,PRECT_lon.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open precip from GPCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcp_filename = '/Users/baird/Dropbox/_data_observations/GPCP/precip.mon.mean.nc'\n",
    "gpcp_ncfile = netCDF4.Dataset(gpcp_filename)\n",
    "gpcp_data = gpcp_ncfile.variables['precip'][:]\n",
    "lat_gpcp = gpcp_ncfile.variables['lat'][:]\n",
    "lon_gpcp = gpcp_ncfile.variables['lon'][:]\n",
    "gpcp_time_var = gpcp_ncfile.variables['time']\n",
    "gpcp_time_data = gpcp_time_var[:]\n",
    "\n",
    "gpcp_time_datetimes = netCDF4.num2date(gpcp_time_data, gpcp_time_var.units, 'standard')\n",
    "gpcp_time_datetimes_subset = [t.year in range(year_start,year_end+1) for t in gpcp_time_datetimes]\n",
    "gpcp_data = gpcp_data[gpcp_time_datetimes_subset,:,:]\n",
    "gpcp_time_datetimes = gpcp_time_datetimes[gpcp_time_datetimes_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of days in each gpcp month\n",
    "month_range = numpy.array(( [calendar.monthrange(t.year,t.month)[1] for t in gpcp_time_datetimes ]))\n",
    "for j in range(gpcp_data.shape[1]):\n",
    "    for k in range(gpcp_data.shape[2]):\n",
    "        gpcp_data[:,j,k] = gpcp_data[:,j,k]*month_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out NDJFM for full years...\n",
    "NDJFM_indices_gpcp = numpy.array(([t.month in [11,12,1,2,3] for t in gpcp_time_datetimes]), dtype=bool)\n",
    "NDJFM_indices_gpcp[0:3]=False\n",
    "NDJFM_indices_gpcp[-2::]=False\n",
    "\n",
    "J_indices_gpcp = numpy.array(([t.month in [1] for t in gpcp_time_datetimes]), dtype=bool)\n",
    "J_indices_gpcp[0]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcp_data_copy = numpy.copy(gpcp_data)\n",
    "#half_index = int(cruts_data_copy.shape[2]/2)\n",
    "#cruts_data_copy[:,:,0:half_index]=cruts_data[:,:,half_index::]\n",
    "#cruts_data_copy[:,:,half_index::]=cruts_data[:,:,0:half_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcp_data_subset = gpcp_data_copy[:,(lat_gpcp>=PRECT_lat.min())&(lat_gpcp<=PRECT_lat.max()),:]\n",
    "gpcp_data_subset = gpcp_data_subset[:,:,(lon_gpcp>=PRECT_lon.min())&(lon_gpcp<=PRECT_lon.max())]\n",
    "lon_gpcp_subset = lon_gpcp[(lon_gpcp>=PRECT_lon.min())&(lon_gpcp<=PRECT_lon.max())]\n",
    "lat_gpcp_subset = lat_gpcp[(lat_gpcp>=PRECT_lat.min())&(lat_gpcp<=PRECT_lat.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seasons = gpcp_time_datetimes[-1].year - gpcp_time_datetimes[0].year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcp_seasonal_totals_array = numpy.zeros((n_seasons,gpcp_data_subset.shape[1],gpcp_data_subset.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcp_seasonal_totals_array_allvals = numpy.zeros((gpcp_data_subset.shape))\n",
    "gpcp_data_subset_nanmonths = numpy.copy(gpcp_data_subset)\n",
    "gpcp_data_subset_nanmonths[NDJFM_indices_gpcp==False,:,:] = numpy.nan\n",
    "for j in range(lat_gpcp_subset.size):\n",
    "    for k in range(lon_gpcp_subset.size):\n",
    "        gpcp_seasonal_totals_array_allvals[:,j,k] = pandas.Series(gpcp_data_subset_nanmonths[:,j,k]).rolling(window=5, center=True).sum()\n",
    "\n",
    "# subset all non-nan values (should come out to 115 time periods, or n_seasons)\n",
    "gpcp_seasonal_totals_array = gpcp_seasonal_totals_array_allvals[J_indices_gpcp,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_combos = numpy.array((list(itertools.product(range(lat_gpcp_subset.size),range(lon_gpcp_subset.size))) ))\n",
    "# choose grid boxes closest to LA / Southern California\n",
    "nanvals_flattened = numpy.copy(gpcp_seasonal_totals_array[0,:,:].reshape(-1))\n",
    "nanvals_flattened[~numpy.isnan(nanvals_flattened)] = 1\n",
    "valid_indices = numpy.where(~numpy.isnan(nanvals_flattened))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_lat_idx_gpcp = geo_idx(LA_lat, lat_gpcp_subset)\n",
    "LA_lon_idx_gpcp = geo_idx(LA_lon, lon_gpcp_subset)\n",
    "\n",
    "SF_lat_idx_gpcp = geo_idx(SF_lat, lat_gpcp_subset)\n",
    "SF_lon_idx_gpcp = geo_idx(SF_lon, lon_gpcp_subset)\n",
    "\n",
    "OD_lat_idx_gpcp = geo_idx(Oroville_dam_lat, lat_gpcp_subset)\n",
    "OD_lon_idx_gpcp = geo_idx(Oroville_dam_lon, lon_gpcp_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=110\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = lat_gpcp_subset[LA_lat_idx_gpcp],lon_gpcp_subset[LA_lon_idx_gpcp]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_gpcp_subset[latlon_combos[latlon][0]], -(360-lon_gpcp_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_gpcp_110km = distance_array_ledist.reshape((lat_gpcp_subset.size,lon_gpcp_subset.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = lat_gpcp_subset[SF_lat_idx_gpcp],lon_gpcp_subset[SF_lon_idx_gpcp]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_gpcp_subset[latlon_combos[latlon][0]], -(360-lon_gpcp_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_gpcp_110km = distance_array_ledist.reshape((lat_gpcp_subset.size,lon_gpcp_subset.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=200\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = lat_gpcp_subset[LA_lat_idx_gpcp],lon_gpcp_subset[LA_lon_idx_gpcp]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_gpcp_subset[latlon_combos[latlon][0]], -(360-lon_gpcp_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_gpcp_200km = distance_array_ledist.reshape((lat_gpcp_subset.size,lon_gpcp_subset.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = lat_gpcp_subset[SF_lat_idx_gpcp],lon_gpcp_subset[SF_lon_idx_gpcp]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_gpcp_subset[latlon_combos[latlon][0]], -(360-lon_gpcp_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_gpcp_200km = distance_array_ledist.reshape((lat_gpcp_subset.size,lon_gpcp_subset.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=300\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = lat_gpcp_subset[LA_lat_idx_gpcp],lon_gpcp_subset[LA_lon_idx_gpcp]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_gpcp_subset[latlon_combos[latlon][0]], -(360-lon_gpcp_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_gpcp_300km = distance_array_ledist.reshape((lat_gpcp_subset.size,lon_gpcp_subset.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = lat_gpcp_subset[SF_lat_idx_gpcp],lon_gpcp_subset[SF_lon_idx_gpcp]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_gpcp_subset[latlon_combos[latlon][0]], -(360-lon_gpcp_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_gpcp_300km = distance_array_ledist.reshape((lat_gpcp_subset.size,lon_gpcp_subset.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open precip from CRU TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruts_filename = '/Users/baird/Dropbox/_analysis/attribution_2017/obs/CRU_TS/cru_ts4.01.1901.2016.pre.dat_ORIG.nc'\n",
    "cruts_ncfile = netCDF4.Dataset(cruts_filename)\n",
    "cruts_data = cruts_ncfile.variables['pre'][:]\n",
    "lon_cruts = cruts_ncfile.variables['lon'][:]+180.\n",
    "lat_cruts = cruts_ncfile.variables['lat'][:]\n",
    "cruts_time_var = cruts_ncfile.variables['time']\n",
    "cruts_time_data = cruts_time_var[:]\n",
    "\n",
    "cruts_time_datetimes = netCDF4.num2date(cruts_time_data, cruts_time_var.units, 'standard')\n",
    "cruts_time_datetimes_subset = [t.year in range(year_start,year_end+1) for t in cruts_time_datetimes]\n",
    "cruts_data = cruts_data[cruts_time_datetimes_subset,:,:]\n",
    "cruts_time_datetimes = cruts_time_datetimes[cruts_time_datetimes_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruts_data_copy = numpy.copy(cruts_data)\n",
    "half_index = int(cruts_data_copy.shape[2]/2)\n",
    "cruts_data_copy[:,:,0:half_index]=cruts_data[:,:,half_index::]\n",
    "cruts_data_copy[:,:,half_index::]=cruts_data[:,:,0:half_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruts_data_copy[cruts_data_copy<=0.1]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruts_data_copy[cruts_data_copy==9.96920997e36]=numpy.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out NDJFM for full years...\n",
    "NDJFM_indices_cruts = numpy.array(([t.month in [11,12,1,2,3] for t in cruts_time_datetimes]), dtype=bool)\n",
    "NDJFM_indices_cruts[0:3]=False\n",
    "NDJFM_indices_cruts[-2::]=False\n",
    "\n",
    "J_indices_cruts = numpy.array(([t.month in [1] for t in cruts_time_datetimes]), dtype=bool)\n",
    "J_indices_cruts[0]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruts_data_subset = cruts_data_copy[:,(lat_cruts>=PRECT_lat.min())&(lat_cruts<=PRECT_lat.max()),:]\n",
    "cruts_data_subset = cruts_data_subset[:,:,(lon_cruts>=PRECT_lon.min())&(lon_cruts<=PRECT_lon.max())]\n",
    "lon_cruts_subset = lon_cruts[(lon_cruts>=PRECT_lon.min())&(lon_cruts<=PRECT_lon.max())]\n",
    "lat_cruts_subset = lat_cruts[(lat_cruts>=PRECT_lat.min())&(lat_cruts<=PRECT_lat.max())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate seasonal totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seasons = cruts_time_datetimes[-1].year - cruts_time_datetimes[0].year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruts_seasonal_totals_array = numpy.zeros((n_seasons,cruts_data_subset.shape[1],cruts_data_subset.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruts_seasonal_totals_array_allvals = numpy.zeros((cruts_data_subset.shape))\n",
    "cruts_data_subset_nanmonths = numpy.copy(cruts_data_subset)\n",
    "cruts_data_subset_nanmonths[NDJFM_indices_cruts==False,:,:] = numpy.nan\n",
    "for j in range(lat_cruts_subset.size):\n",
    "    for k in range(lon_cruts_subset.size):\n",
    "        cruts_seasonal_totals_array_allvals[:,j,k] = pandas.Series(cruts_data_subset_nanmonths[:,j,k]).rolling(window=5, center=True).sum()\n",
    "\n",
    "# subset all non-nan values (should come out to 115 time periods, or n_seasons)\n",
    "cruts_seasonal_totals_array = cruts_seasonal_totals_array_allvals[J_indices_cruts,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_combos = numpy.array((list(itertools.product(range(lat_cruts_subset.size),range(lon_cruts_subset.size))) ))\n",
    "# choose grid boxes closest to LA / Southern California\n",
    "lat,lon = lat_cruts_subset[latlon_combos[2000][0]], lon_cruts_subset[latlon_combos[2000][1]]\n",
    "nanvals_flattened = numpy.copy(cruts_seasonal_totals_array[0,:,:].reshape(-1))\n",
    "nanvals_flattened[~numpy.isnan(nanvals_flattened)] = 1\n",
    "valid_indices = numpy.where(~numpy.isnan(nanvals_flattened))[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "location_list = []\n",
    "geolocator = geopy.geocoders.Nominatim()\n",
    "for latlon in valid_indices:#latlon_combos.__len__()):\n",
    "    lat,lon = lat_cruts_subset[latlon_combos[latlon][0]],lon_cruts_subset[latlon_combos[latlon][1]]\n",
    "    location_list.append( geolocator.reverse((lat,-(360-lon))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_lat_idx_cruts = geo_idx(LA_lat, lat_cruts_subset)\n",
    "LA_lon_idx_cruts = geo_idx(LA_lon, lon_cruts_subset)\n",
    "\n",
    "SF_lat_idx_cruts = geo_idx(SF_lat, lat_cruts_subset)\n",
    "SF_lon_idx_cruts = geo_idx(SF_lon, lon_cruts_subset)\n",
    "\n",
    "OD_lat_idx_cruts = geo_idx(Oroville_dam_lat, lat_cruts_subset)\n",
    "OD_lon_idx_cruts = geo_idx(Oroville_dam_lon, lon_cruts_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=110\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = lat_cruts_subset[LA_lat_idx_cruts],lon_cruts_subset[LA_lon_idx_cruts]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_cruts_subset[latlon_combos[latlon][0]], -(360-lon_cruts_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_cruts_110km = distance_array_ledist.reshape((lat_cruts_subset.size,lon_cruts_subset.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = lat_cruts_subset[SF_lat_idx_cruts],lon_cruts_subset[SF_lon_idx_cruts]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_cruts_subset[latlon_combos[latlon][0]], -(360-lon_cruts_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_cruts_110km = distance_array_ledist.reshape((lat_cruts_subset.size,lon_cruts_subset.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=200\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = lat_cruts_subset[LA_lat_idx_cruts],lon_cruts_subset[LA_lon_idx_cruts]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_cruts_subset[latlon_combos[latlon][0]], -(360-lon_cruts_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_cruts_200km = distance_array_ledist.reshape((lat_cruts_subset.size,lon_cruts_subset.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = lat_cruts_subset[SF_lat_idx_cruts],lon_cruts_subset[SF_lon_idx_cruts]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_cruts_subset[latlon_combos[latlon][0]], -(360-lon_cruts_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_cruts_200km = distance_array_ledist.reshape((lat_cruts_subset.size,lon_cruts_subset.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "dist=300\n",
    "\n",
    "# FOR LOS ANGELES\n",
    "lat0,lon0 = lat_cruts_subset[LA_lat_idx_cruts],lon_cruts_subset[LA_lon_idx_cruts]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_cruts_subset[latlon_combos[latlon][0]], -(360-lon_cruts_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_LA_2d_cruts_300km = distance_array_ledist.reshape((lat_cruts_subset.size,lon_cruts_subset.size))\n",
    "\n",
    "\n",
    "# FOR SAN FRANCISCO\n",
    "lat0,lon0 = lat_cruts_subset[SF_lat_idx_cruts],lon_cruts_subset[SF_lon_idx_cruts]\n",
    "point0 = (lat0,-(360-lon0))\n",
    "\n",
    "distance_list = []\n",
    "for latlon in range(latlon_combos.__len__()):\n",
    "    lat_new,lon_new = lat_cruts_subset[latlon_combos[latlon][0]], -(360-lon_cruts_subset[latlon_combos[latlon][1]])\n",
    "    distance_list.append( geopy.distance.vincenty(point0, (lat_new,lon_new)).km )\n",
    "distance_array = numpy.array(distance_list)\n",
    "distance_array_ledist = numpy.copy(distance_array)\n",
    "distance_array_ledist[distance_array>dist]=numpy.nan\n",
    "distance_array_ledist[distance_array<=dist]=1\n",
    "distance_array_SF_2d_cruts_300km = distance_array_ledist.reshape((lat_cruts_subset.size,lon_cruts_subset.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get landmask for CRU TS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mp.pcolormesh(cruts_seasonal_totals_array[0,:,:])\n",
    "cruts_landmask = ~numpy.isnan(cruts_seasonal_totals_array[0,:,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cf=mp.contourf(cruts_landmask)\n",
    "mp.colorbar(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get landmask for LENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmask_file = '/Users/baird/Dropbox/Precipitation Extremes Project/Whiplash_manuscript/topography_LENS/USGS-gtopo30_0.9x1.25_remap_c051027.nc'\n",
    "landmask_ncfile = netCDF4.Dataset(landmask_file)\n",
    "landfrac = landmask_ncfile.variables['LANDFRAC'][:]\n",
    "landfrac_lat = landmask_ncfile.variables['lat'][:]\n",
    "landfrac_lon = landmask_ncfile.variables['lon'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "landfrac_lat_subset = (landfrac_lat<=PRECT_lat.max()) & (landfrac_lat>=(PRECT_lat.min()-0.5))\n",
    "landfrac_lon_subset = (landfrac_lon<=PRECT_lon.max()) & (landfrac_lon>=PRECT_lon.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baird/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "landfrac_subset = landfrac[landfrac_lat_subset,:]\n",
    "landfrac_subset = landfrac_subset[:,landfrac_lon_subset]\n",
    "landfrac_nanvals = numpy.ones(landfrac_subset.shape)\n",
    "landfrac_nanvals[landfrac_subset==0] = numpy.nan\n",
    "landfrac_nanvals[landfrac_nanvals>0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get landmask for GPCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmask_file = '/Users/baird/Dropbox/_analysis/obs_analysis/OBS_landfrac_oceanfrac_2.5x2.5.nc'\n",
    "landmask_ncfile = netCDF4.Dataset(landmask_file)\n",
    "landsea = landmask_ncfile.variables['landsea_int'][:]\n",
    "landsea_lat = landmask_ncfile.variables['lat'][:]\n",
    "landsea_lon = landmask_ncfile.variables['lon'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsea_lat_subset = (landsea_lat<=PRECT_lat.max()+10) & (landsea_lat>=(PRECT_lat.min()-10))\n",
    "landsea_lon_subset = (landsea_lon<=PRECT_lon.max()+10) & (landsea_lon>=PRECT_lon.min()-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baird/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "landsea_subset = landsea[landsea_lat_subset,:]\n",
    "landsea_subset = landsea_subset[:,landsea_lon_subset]\n",
    "landsea_nanvals = numpy.ones(landsea_subset.shape)\n",
    "landsea_nanvals[landsea_subset==0] = numpy.nan\n",
    "landsea_nanvals[landsea_nanvals>0] = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig,ax = mp.subplots(ncols=1,nrows=1,subplot_kw={'projection':cartopy.crs.PlateCarree()})\n",
    "ax.pcolormesh(landsea_lon[landsea_lon_subset],landsea_lat[landsea_lat_subset],landsea_nanvals)\n",
    "ax.coastlines()\n",
    "ax.set_extent([-180,0,0,60])#[PRECT_lon.min(),PRECT_lon.max(),PRECT_lat.min(),PRECT_lat.max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pool means from the appropriate regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for LENS data (DON'T RESHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_LA_lens_totals_110km = hist_rcp_data*distance_array_LA_2d_lens_110km*landfrac_nanvals\n",
    "collect_SF_lens_totals_110km = hist_rcp_data*distance_array_SF_2d_lens_110km*landfrac_nanvals\n",
    "\n",
    "area_mean_LA_lens_totals_110km = numpy.nanmean(collect_LA_lens_totals_110km, axis=(2,3))\n",
    "area_mean_SF_lens_totals_110km = numpy.nanmean(collect_SF_lens_totals_110km, axis=(2,3))\n",
    "\n",
    "#rand_int = numpy.random.randint(low=0,high=area_mean_LA_lens_totals.size,size=115)\n",
    "#rand_int = numpy.arange(area_mean_LA_lens_totals_110km.size)\n",
    "\n",
    "#area_mean_LA_lens_totals_110km = area_mean_LA_lens_totals_110km[rand_int]\n",
    "#area_mean_SF_lens_totals_110km = area_mean_SF_lens_totals_110km[rand_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 40, 26, 25)\n"
     ]
    }
   ],
   "source": [
    "print(hist_rcp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_LA_lens_totals_200km = hist_rcp_data*distance_array_LA_2d_lens_200km*landfrac_nanvals\n",
    "collect_SF_lens_totals_200km = hist_rcp_data*distance_array_SF_2d_lens_200km*landfrac_nanvals\n",
    "\n",
    "area_mean_LA_lens_totals_200km = numpy.nanmean(collect_LA_lens_totals_200km, axis=(2,3))#.reshape(-1)\n",
    "area_mean_SF_lens_totals_200km = numpy.nanmean(collect_SF_lens_totals_200km, axis=(2,3))#.reshape(-1)\n",
    "\n",
    "#rand_int = numpy.random.randint(low=0,high=area_mean_LA_lens_totals.size,size=115)\n",
    "\n",
    "#area_mean_LA_lens_totals_200km = area_mean_LA_lens_totals_200km[rand_int]\n",
    "#area_mean_SF_lens_totals_200km = area_mean_SF_lens_totals_200km[rand_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_LA_lens_totals_300km = hist_rcp_data*distance_array_LA_2d_lens_300km*landfrac_nanvals\n",
    "collect_SF_lens_totals_300km = hist_rcp_data*distance_array_SF_2d_lens_300km*landfrac_nanvals\n",
    "\n",
    "area_mean_LA_lens_totals_300km = numpy.nanmean(collect_LA_lens_totals_300km, axis=(2,3))#.reshape(-1)\n",
    "area_mean_SF_lens_totals_300km = numpy.nanmean(collect_SF_lens_totals_300km, axis=(2,3))#.reshape(-1)\n",
    "\n",
    "#rand_int = numpy.random.randint(low=0,high=area_mean_LA_lens_totals.size,size=115)\n",
    "\n",
    "#area_mean_LA_lens_totals_300km = area_mean_LA_lens_totals_300km[rand_int]\n",
    "#area_mean_SF_lens_totals_300km = area_mean_SF_lens_totals_300km[rand_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save 200km distribution info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 110, 200, 300km column for GPCP, CRU TS, and LENS separately\n",
    "\n",
    "column_headers = ['LA_300km_mm_total', 'LA_200km_mm_total', 'LA_100km_mm_total', \\\n",
    "                  'SF_300km_mm_total', 'SF_200km_mm_total', 'SF_100km_mm_total']\n",
    "\n",
    "# area_mean_LA_lens_totals_300km\n",
    "lens_array = numpy.column_stack(( area_mean_LA_lens_totals_110km, area_mean_LA_lens_totals_200km, area_mean_LA_lens_totals_300km ,\\\n",
    "                                area_mean_SF_lens_totals_110km, area_mean_SF_lens_totals_200km, area_mean_SF_lens_totals_300km ))\n",
    "lens_df = pandas.DataFrame(lens_array,columns=column_headers)\n",
    "lens_df.to_csv('LENS_area_mean_seasonal_totals_'+ys_str+'-'+ye_str+'.csv')\n",
    "\n",
    "cruts_array = numpy.column_stack(( area_mean_LA_cruts_totals_110km, area_mean_LA_cruts_totals_200km, area_mean_LA_cruts_totals_300km ,\\\n",
    "                                area_mean_SF_cruts_totals_110km, area_mean_SF_cruts_totals_200km, area_mean_SF_cruts_totals_300km ))\n",
    "cruts_df = pandas.DataFrame(cruts_array,columns=column_headers)\n",
    "cruts_df.to_csv('CRUTS_area_mean_seasonal_totals_'+ys_str+'-'+ye_str+'.csv')\n",
    "\n",
    "gpcp_array = numpy.column_stack(( area_mean_LA_gpcp_totals_110km, area_mean_LA_gpcp_totals_200km, area_mean_LA_gpcp_totals_300km ,\\\n",
    "                                area_mean_SF_gpcp_totals_110km, area_mean_SF_gpcp_totals_200km, area_mean_SF_gpcp_totals_300km ))\n",
    "gpcp_df = pandas.DataFrame(gpcp_array,columns=column_headers)\n",
    "gpcp_df.to_csv('GPCP_area_mean_seasonal_totals_'+ys_str+'-'+ye_str+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = ['{:03d}'.format(i) for i in list(range(1,36))+list(range(101,106))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_df = pandas.DataFrame(area_mean_LA_lens_totals_200km,columns=column_headers)\n",
    "lens_df.to_csv('LENS_area_mean_seasonal_totals_'+ys_str+'-'+ye_str+'_individual_ens_members_LA.csv')\n",
    "\n",
    "lens_df = pandas.DataFrame(area_mean_SF_lens_totals_200km,columns=column_headers)\n",
    "lens_df.to_csv('LENS_area_mean_seasonal_totals_'+ys_str+'-'+ye_str+'_individual_ens_members_SF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
