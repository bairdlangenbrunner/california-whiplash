{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import netCDF4\n",
    "import matplotlib.pyplot as mp\n",
    "import matplotlib.ticker\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "mp.rcParams.update({'mathtext.default': 'regular'})\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_idx(dd, dd_array):\n",
    "   \"\"\"\n",
    "     search for nearest decimal degree in an array of decimal degrees and return the index.\n",
    "     np.argmin returns the indices of minium value along an axis.\n",
    "     so subtract dd from all values in dd_array, take absolute value and find index of minium.\n",
    "    \"\"\"\n",
    "   geo_idx = (numpy.abs(dd_array - dd)).argmin()\n",
    "   return geo_idx\n",
    "\n",
    "LA_lat = 34.0522\n",
    "LA_lon = 118.2437 # deg west\n",
    "LA_lon = 180. + (180-LA_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_subsets = numpy.array((\n",
    "[1920,1940], \\\n",
    "[1940,1960], \\\n",
    "[1960,1980], \\\n",
    "[1980,2000], \\\n",
    "[2000,2020], \\\n",
    "[2020,2040], \\\n",
    "[2040,2060], \\\n",
    "[2060,2080], \\\n",
    "[2080,2100] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_dir = '/ninod/NCAR_LENS/daily/PRECT/B20TRC5CNBDRD/'\n",
    "\n",
    "file_list = numpy.array(( \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.001.cam.h1.PRECT.18500101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.002.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.003.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.004.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.005.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.006.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.007.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.008.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.009.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.010.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.011.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.012.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.013.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.014.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.015.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.016.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.017.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.018.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.019.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.020.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.021.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.022.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.023.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.024.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.025.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.026.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.027.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.028.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.029.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.030.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.031.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.032.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.033.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.034.cam.h1.PRECT.19200101-20051231.nc', \\\n",
    "'b.e11.B20TRC5CNBDRD.f09_g16.035.cam.h1.PRECT.19200101-20051231.nc' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_dir_rcp = '/ninod/NCAR_LENS/daily/PRECT/BRCP85C5CNBDRD/'\n",
    "\n",
    "file_list_rcp = numpy.array(( \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.001.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.002.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.003.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.004.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.005.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.006.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.007.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.008.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.009.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.010.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.011.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.012.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.013.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.014.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.015.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.016.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.017.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.018.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.019.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.020.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.021.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.022.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.023.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.024.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.025.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.026.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.027.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.028.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.029.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.030.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.031.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.032.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.033.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.034.cam.h1.PRECT.20060101-21001231.nc', \\\n",
    "'b.e11.BRCP85C5CNBDRD.f09_g16.035.cam.h1.PRECT.20060101-21001231.nc' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out proper lat/lon (doing this for one grid point right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_oct = 31\n",
    "d_nov = 30\n",
    "d_dec = 31\n",
    "d_jan = 31\n",
    "d_feb = 28\n",
    "d_mar = 31\n",
    "days_per_season = d_oct+d_nov+d_dec+d_jan+d_feb+d_mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "ncfile = netCDF4.Dataset(working_dir + file_list[i])\n",
    "PRECT_lat = ncfile.variables['lat'][:]\n",
    "PRECT_lon = ncfile.variables['lon'][:]\n",
    "PRECT_time_var = ncfile.variables['time']\n",
    "\n",
    "PRECT_time_dates = netCDF4.num2date(PRECT_time_var[:], PRECT_time_var.units, PRECT_time_var.calendar)\n",
    "time_indices_ONDJFM = numpy.array([t.month in [10,11,12,1,2,3] for t in PRECT_time_dates], dtype=bool)\n",
    "PRECT_time_dates_ONDJFM = PRECT_time_dates[time_indices_ONDJFM]\n",
    "\n",
    "LA_lat_idx = geo_idx(LA_lat, PRECT_lat)\n",
    "LA_lon_idx = geo_idx(LA_lon, PRECT_lon)\n",
    "\n",
    "#PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,(LA_lat_idx-2):(LA_lat_idx+2), (LA_lon_idx-2):(LA_lon_idx+2)]*86400.*1000\n",
    "PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,LA_lat_idx, LA_lon_idx]*86400.*1000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#PRECT_ONDJFM = PRECT_ONDJFM[:, LA_lat_idx, LA_lon_idx]\n",
    "PRECT_ONDJFM_CA = PRECT_ONDJFM[:, (LA_lat_idx-2):(LA_lat_idx+2), (LA_lon_idx-2):(LA_lon_idx+2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up multiprocessing stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For chunks 3 and below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list_subset = file_list[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1\n",
      "{'_jointhread': None, '_wlock': <Lock(owner=None)>, '_close': None, '_maxsize': 2147483647, '_joincancelled': False, '_opid': 23507, '_sem': <BoundedSemaphore(value=2147483647, maxvalue=2147483647)>, '_reader': <multiprocessing.connection.Connection object at 0x7f3e2c5104e0>, '_buffer': deque([]), '_recv_bytes': <bound method _ConnectionBase.recv_bytes of <multiprocessing.connection.Connection object at 0x7f3e2c5104e0>>, '_rlock': <Lock(owner=None)>, '_poll': <bound method _ConnectionBase.poll of <multiprocessing.connection.Connection object at 0x7f3e2c5104e0>>, '_thread': None, '_ignore_epipe': False, '_writer': <multiprocessing.connection.Connection object at 0x7f3e2c510518>, '_send_bytes': <bound method _ConnectionBase.send_bytes of <multiprocessing.connection.Connection object at 0x7f3e2c510518>>, '_closed': False, '_notempty': <Condition(<unlocked _thread.lock object at 0x7f3e2c58f850>, 0)>}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Queue' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-996292fbea94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m#print(queue)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Queue' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #pool = multiprocessing.Pool()\n",
    "    queue = multiprocessing.Queue()\n",
    "    process = multiprocessing.Process(target=calculate_chunk, args=(file_list, queue))\n",
    "    #process.start()\n",
    "    #input = file_list, queue\n",
    "    #results = pool.map(calculate_chunk, input)\n",
    "    return(queue)\n",
    "\n",
    "def calculate_chunk(file_list, results_queue):\n",
    "    \n",
    "    file = file_list\n",
    "    print(file)\n",
    "\n",
    "    ncfile = netCDF4.Dataset(working_dir + file)\n",
    "    PRECT_lat = ncfile.variables['lat'][:]\n",
    "    PRECT_lon = ncfile.variables['lon'][:]\n",
    "    PRECT_time_var = ncfile.variables['time']\n",
    "\n",
    "    PRECT_time_dates = netCDF4.num2date(PRECT_time_var[:], PRECT_time_var.units, PRECT_time_var.calendar)\n",
    "    time_indices_ONDJFM = numpy.array([(t.month in [10,11,12,1,2,3])&(t.year in range(year_start,year_end+1)) for t in PRECT_time_dates], dtype=bool)\n",
    "    PRECT_time_dates_ONDJFM = PRECT_time_dates[time_indices_ONDJFM]\n",
    "\n",
    "    LA_lat_idx = geo_idx(LA_lat, PRECT_lat)\n",
    "    LA_lon_idx = geo_idx(LA_lon, PRECT_lon)\n",
    "\n",
    "    #PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,(LA_lat_idx-2):(LA_lat_idx+2), (LA_lon_idx-2):(LA_lon_idx+2)]*86400.*1000\n",
    "    PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,LA_lat_idx, LA_lon_idx]*86400.*1000\n",
    "\n",
    "    # get index of first October\n",
    "    # and index of last March\n",
    "\n",
    "    t=0\n",
    "    while t < PRECT_time_dates_ONDJFM.size:\n",
    "        if PRECT_time_dates_ONDJFM[t].month!=10:\n",
    "            t+=1\n",
    "        else:\n",
    "            first_October_index = t\n",
    "            break\n",
    "    t=0\n",
    "    while i < PRECT_time_dates_ONDJFM.size:\n",
    "        if PRECT_time_dates_ONDJFM[::-1][t].month!=3:\n",
    "            t+=1\n",
    "        else:\n",
    "            last_March_index = PRECT_time_dates_ONDJFM.size-t-1\n",
    "            break\n",
    "\n",
    "    PRECT_time_dates_ONDJFM_fullseasons = PRECT_time_dates_ONDJFM[first_October_index:last_March_index+1]\n",
    "    PRECT_ONDJFM_CA_fullseasons = PRECT_ONDJFM_CA[first_October_index:last_March_index+1]\n",
    "\n",
    "    years_array = numpy.array(([t.year for t in PRECT_time_dates_ONDJFM_fullseasons]), dtype=numpy.int)\n",
    "    years_array_unique = numpy.unique(years_array)\n",
    "\n",
    "    n_seasons = numpy.int(PRECT_time_dates_ONDJFM_fullseasons.size/days_per_season)\n",
    "    #print(n_seasons)\n",
    "\n",
    "    storm_count_list=[] # for each season\n",
    "    storm_length_list=[] # for each storm, in days\n",
    "    storm_magnitude_list=[] # for each storm, in mm/day\n",
    "\n",
    "    storm_count=0 # per season\n",
    "    storm_length=0 # for each storm\n",
    "    storm_magnitude=0.0 # for each storm\n",
    "\n",
    "    for s in range(n_seasons):\n",
    "        seas_PRECT = PRECT_ONDJFM_CA_fullseasons[s*days_per_season:(s*days_per_season+days_per_season)]\n",
    "        for d in range(days_per_season):\n",
    "            if seas_PRECT[d]>=threshold:\n",
    "                storm_magnitude+=seas_PRECT[d]\n",
    "                storm_length+=1 # days\n",
    "            elif (seas_PRECT[d]<threshold)&(storm_magnitude>0.0):\n",
    "                storm_magnitude_list.append(storm_magnitude)\n",
    "                storm_length_list.append(storm_length)\n",
    "                storm_magnitude=0.0\n",
    "                storm_length=0\n",
    "                storm_count+=1\n",
    "        storm_count_list.append(storm_count)\n",
    "        #print(storm_count)\n",
    "        storm_count=0\n",
    "    \n",
    "    results_queue.put(storm_count_list, storm_length_list, storm_magnitude_list)\n",
    "    #print(results_queue)\n",
    "\n",
    "for chunk in [0,1,2,3]:\n",
    "    \n",
    "    print(\"chunk \"+str(chunk+1))\n",
    "    year_start = time_subsets[chunk,0]\n",
    "    year_end = time_subsets[chunk,1]\n",
    "    return_queue = main()\n",
    "    \n",
    "    print(vars(return_queue))\n",
    "    print(return_queue.values())\n",
    "\n",
    "    #print(queue)\n",
    "    # print(len(storm_count_list))\n",
    "    #dictionary = {\\\n",
    "    #'storm_count_list':storm_count_list, \\\n",
    "    #'storm_length_list':storm_length_list, \\\n",
    "    #'storm_magnitude_list':storm_magnitude_list }\n",
    "\n",
    "    #save_dir = '/ninod/baird/cmip5/cmip5_calculations/attribution_2017/storm_counting/'\n",
    "    #numpy.save(save_dir + 'storm_counting_decadal_chunks_'+str(year_start)+'-'+str(year_end)+'_threshold_'+str(threshold)+'mmday.npy', dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For chunk 4 (must combine two data sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    pool = multiprocessing.Pool()\n",
    "    input = [file_list, file_list_rcp]\n",
    "    pool.map(calculate_chunk, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk = 4\n",
    "\n",
    "year_start = time_subsets[chunk,0]\n",
    "year_end = time_subsets[chunk,1]\n",
    "\n",
    "#fig = mp.figure(figsize=(18,9))\n",
    "\n",
    "for i in range(file_list.size):\n",
    "\n",
    "    file, file_rcp = input\n",
    "\n",
    "    # OPEN HISTORICAL\n",
    "    ncfile = netCDF4.Dataset(working_dir + file)\n",
    "    PRECT_lat = ncfile.variables['lat'][:]\n",
    "    PRECT_lon = ncfile.variables['lon'][:]\n",
    "    PRECT_time_var = ncfile.variables['time']\n",
    "\n",
    "    PRECT_time_dates = netCDF4.num2date(PRECT_time_var[:], PRECT_time_var.units, PRECT_time_var.calendar)\n",
    "    time_indices_ONDJFM = numpy.array([(t.month in [10,11,12,1,2,3])&(t.year >= year_start) for t in PRECT_time_dates], dtype=bool)\n",
    "    PRECT_time_dates_ONDJFM = PRECT_time_dates[time_indices_ONDJFM]\n",
    "    LA_lat_idx = geo_idx(LA_lat, PRECT_lat)\n",
    "    LA_lon_idx = geo_idx(LA_lon, PRECT_lon)\n",
    "\n",
    "    #PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,(LA_lat_idx-2):(LA_lat_idx+2), (LA_lon_idx-2):(LA_lon_idx+2)]*86400.*1000\n",
    "    PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,LA_lat_idx, LA_lon_idx]*86400.*1000\n",
    "\n",
    "    # OPEN RCP8.5\n",
    "    ncfile = netCDF4.Dataset(working_dir_rcp + file_rcp)\n",
    "    PRECT_time_var = ncfile.variables['time']\n",
    "    PRECT_time_dates_rcp = netCDF4.num2date(PRECT_time_var[:], PRECT_time_var.units, PRECT_time_var.calendar)\n",
    "    time_indices_ONDJFM_rcp = numpy.array([(t.month in [10,11,12,1,2,3])&(t.year < year_end+1) for t in PRECT_time_dates_rcp], dtype=bool)\n",
    "    PRECT_time_dates_ONDJFM_rcp = PRECT_time_dates_rcp[time_indices_ONDJFM_rcp]\n",
    "    PRECT_ONDJFM_CA_rcp = ncfile.variables['PRECT'][time_indices_ONDJFM_rcp,LA_lat_idx, LA_lon_idx]*86400.*1000\n",
    "    \n",
    "    # COMBINE THEM\n",
    "    PRECT_ONDJFM_CA = numpy.hstack((PRECT_ONDJFM_CA, PRECT_ONDJFM_CA_rcp))\n",
    "    PRECT_time_dates_ONDJFM = numpy.hstack((PRECT_time_dates_ONDJFM, PRECT_time_dates_ONDJFM_rcp))\n",
    "    print(PRECT_ONDJFM_CA.shape)\n",
    "    \n",
    "    print(PRECT_time_dates_ONDJFM[-1])\n",
    "    print(PRECT_time_dates_ONDJFM_rcp[0])\n",
    "    # get index of first October\n",
    "    # and index of last March\n",
    "    t=0\n",
    "    while t < PRECT_time_dates_ONDJFM.size:\n",
    "        if PRECT_time_dates_ONDJFM[t].month!=10:\n",
    "            t+=1\n",
    "        else:\n",
    "            first_October_index = t\n",
    "            break\n",
    "    t=0\n",
    "    while i < PRECT_time_dates_ONDJFM.size:\n",
    "        if PRECT_time_dates_ONDJFM[::-1][t].month!=3:\n",
    "            t+=1\n",
    "        else:\n",
    "            last_March_index = PRECT_time_dates_ONDJFM.size-t-1\n",
    "            break\n",
    "\n",
    "    PRECT_time_dates_ONDJFM_fullseasons = PRECT_time_dates_ONDJFM[first_October_index:last_March_index+1]\n",
    "    PRECT_ONDJFM_CA_fullseasons = PRECT_ONDJFM_CA[first_October_index:last_March_index+1]\n",
    "\n",
    "    years_array = numpy.array(([t.year for t in PRECT_time_dates_ONDJFM_fullseasons]), dtype=numpy.int)\n",
    "    years_array_unique = numpy.unique(years_array)\n",
    "\n",
    "    n_seasons = numpy.int(PRECT_time_dates_ONDJFM_fullseasons.size/days_per_season)\n",
    "\n",
    "    storm_count_list=[] # for each season\n",
    "    storm_length_list=[] # for each storm, in days\n",
    "    storm_magnitude_list=[] # for each storm, in mm/day\n",
    "\n",
    "    storm_count=0 # per season\n",
    "    storm_length=0 # for each storm\n",
    "    storm_magnitude=0.0 # for each storm\n",
    "\n",
    "    for s in range(n_seasons):\n",
    "        seas_PRECT = PRECT_ONDJFM_CA_fullseasons[s*days_per_season:(s*days_per_season+days_per_season)]\n",
    "        for d in range(days_per_season):\n",
    "            if seas_PRECT[d]>=threshold:\n",
    "                storm_magnitude+=seas_PRECT[d]\n",
    "                storm_length+=1 # days\n",
    "            elif (seas_PRECT[d]<threshold)&(storm_magnitude>0.0):\n",
    "                storm_magnitude_list.append(storm_magnitude)\n",
    "                storm_length_list.append(storm_length)\n",
    "                storm_magnitude=0.0\n",
    "                storm_length=0\n",
    "                storm_count+=1\n",
    "        storm_count_list.append(storm_count)\n",
    "        storm_count=0\n",
    "\n",
    "dictionary = {\\\n",
    "'storm_count_list':storm_count_list, \\\n",
    "'storm_length_list':storm_length_list, \\\n",
    "'storm_magnitude_list':storm_magnitude_list }\n",
    "\n",
    "save_dir = '/ninod/baird/cmip5/cmip5_calculations/attribution_2017/storm_counting/'\n",
    "numpy.save(save_dir + 'storm_counting_decadal_chunks_'+str(year_start)+'-'+str(year_end)+'_threshold_'+str(threshold)+'mmday.npy', dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### For all chunks 5 and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig = mp.figure(figsize=(18,9))\n",
    "\n",
    "for chunk in [5,6,7,8]:\n",
    "\n",
    "    year_start = time_subsets[chunk,0]\n",
    "    year_end = time_subsets[chunk,1]\n",
    "\n",
    "    for i in range(file_list_rcp.size):\n",
    "\n",
    "        print(file_list_rcp[i])\n",
    "\n",
    "        ncfile = netCDF4.Dataset(working_dir_rcp + file_list_rcp[i])\n",
    "        PRECT_lat = ncfile.variables['lat'][:]\n",
    "        PRECT_lon = ncfile.variables['lon'][:]\n",
    "        PRECT_time_var = ncfile.variables['time']\n",
    "\n",
    "        PRECT_time_dates = netCDF4.num2date(PRECT_time_var[:], PRECT_time_var.units, PRECT_time_var.calendar)\n",
    "        time_indices_ONDJFM = numpy.array([(t.month in [10,11,12,1,2,3])&(t.year in range(year_start,year_end+1)) for t in PRECT_time_dates], dtype=bool)\n",
    "        PRECT_time_dates_ONDJFM = PRECT_time_dates[time_indices_ONDJFM]\n",
    "\n",
    "        LA_lat_idx = geo_idx(LA_lat, PRECT_lat)\n",
    "        LA_lon_idx = geo_idx(LA_lon, PRECT_lon)\n",
    "\n",
    "        #PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,(LA_lat_idx-2):(LA_lat_idx+2), (LA_lon_idx-2):(LA_lon_idx+2)]*86400.*1000\n",
    "        PRECT_ONDJFM_CA = ncfile.variables['PRECT'][time_indices_ONDJFM,LA_lat_idx, LA_lon_idx]*86400.*1000\n",
    "\n",
    "        # get index of first October\n",
    "        # and index of last March\n",
    "\n",
    "        t=0\n",
    "        while t < PRECT_time_dates_ONDJFM.size:\n",
    "            if PRECT_time_dates_ONDJFM[t].month!=10:\n",
    "                t+=1\n",
    "            else:\n",
    "                first_October_index = t\n",
    "                break\n",
    "        t=0\n",
    "        while i < PRECT_time_dates_ONDJFM.size:\n",
    "            if PRECT_time_dates_ONDJFM[::-1][t].month!=3:\n",
    "                t+=1\n",
    "            else:\n",
    "                last_March_index = PRECT_time_dates_ONDJFM.size-t-1\n",
    "                break\n",
    "\n",
    "        PRECT_time_dates_ONDJFM_fullseasons = PRECT_time_dates_ONDJFM[first_October_index:last_March_index+1]\n",
    "        PRECT_ONDJFM_CA_fullseasons = PRECT_ONDJFM_CA[first_October_index:last_March_index+1]\n",
    "\n",
    "        years_array = numpy.array(([t.year for t in PRECT_time_dates_ONDJFM_fullseasons]), dtype=numpy.int)\n",
    "        years_array_unique = numpy.unique(years_array)\n",
    "\n",
    "        n_seasons = numpy.int(PRECT_time_dates_ONDJFM_fullseasons.size/days_per_season)\n",
    "\n",
    "        storm_count_list=[] # for each season\n",
    "        storm_length_list=[] # for each storm, in days\n",
    "        storm_magnitude_list=[] # for each storm, in mm/day\n",
    "\n",
    "        storm_count=0 # per season\n",
    "        storm_length=0 # for each storm\n",
    "        storm_magnitude=0.0 # for each storm\n",
    "\n",
    "        for s in range(n_seasons):\n",
    "            seas_PRECT = PRECT_ONDJFM_CA_fullseasons[s*days_per_season:(s*days_per_season+days_per_season)]\n",
    "            for d in range(days_per_season):\n",
    "                if seas_PRECT[d]>=threshold:\n",
    "                    storm_magnitude+=seas_PRECT[d]\n",
    "                    storm_length+=1 # days\n",
    "                elif (seas_PRECT[d]<threshold)&(storm_magnitude>0.0):\n",
    "                    storm_magnitude_list.append(storm_magnitude)\n",
    "                    storm_length_list.append(storm_length)\n",
    "                    storm_magnitude=0.0\n",
    "                    storm_length=0\n",
    "                    storm_count+=1\n",
    "            storm_count_list.append(storm_count)\n",
    "            storm_count=0\n",
    "\n",
    "\n",
    "    dictionary = {\\\n",
    "    'storm_count_list':storm_count_list, \\\n",
    "    'storm_length_list':storm_length_list, \\\n",
    "    'storm_magnitude_list':storm_magnitude_list }\n",
    "\n",
    "    save_dir = '/ninod/baird/cmip5/cmip5_calculations/attribution_2017/storm_counting/'\n",
    "    numpy.save(save_dir + 'storm_counting_decadal_chunks_'+str(year_start)+'-'+str(year_end)+'_threshold_'+str(threshold)+'mmday.npy', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
